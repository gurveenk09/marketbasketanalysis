import pandas as pd

df = pd.read_csv('ls_new.csv')

df
df.info()
# Convert 'date' column to datetime
df['Date'] = pd.to_datetime(df['Date'])
#checking if it changed to datetime
df.info()
df.head()
# Sort the DataFrame by 'date' in ascending order
df = df.sort_values(by='Date')
df
### Creating a subset for school holidays
# Create subset for dates between 13th April and 28th April
start_date = '2024-04-13'
end_date = '2024-04-28'

school_holidays = df[(df['Date'] >= start_date) & (df['Date'] <= end_date)]

school_holidays
#trying to find the number of transactions during last school holidays
school_holidays['ReceiptNumber'].nunique()
items_to_remove = ['Paper Checkout Bags','Discount', 'Clearance']

filtered_df = school_holidays[~school_holidays['Details'].isin(items_to_remove)]
filtered_df.shape
# Ensure 'Quantity' is integer and handle NaNs
filtered_df['Quantity'] = filtered_df['Quantity'].fillna(0).astype(int)

# Repeat 'Details' by 'Quantity' to calculate the real item quantity sold in each row
repeated_details = filtered_df.apply(lambda row: [row['Details']] * row['Quantity'], axis=1)

# Flatten the list of lists
flattened_details = [item for sublist in repeated_details for item in sublist]

# Calculate value counts
details_counts = pd.Series(flattened_details).value_counts().reset_index()

details_counts.columns = ['NameofProduct', 'PurchaseFrequencyDuringHolidays']
details_counts['PurchaseFrequencyDuringHolidays'].sum()
details_counts.to_csv('HighestProductsSoldDuringHolidays.csv')
sales_line_df = filtered_df[['ReceiptNumber', 'Details']]
# Convert the transaction data into a one-hot encoded format
# Get unique items
unique_items = sales_line_df['Details'].unique()
# Transform to Table Format (One-Hot Encoding)
df_retail_txn_table = sales_line_df.pivot_table(index='ReceiptNumber', columns='Details', aggfunc='size', fill_value=0)
df_retail_txn_table
# You can see we have the count, which may be >1 for some cases. We'll change this to binary to align with the format.
df_retail_txn_table = (df_retail_txn_table > 0).astype(int) # This one sets all 1+ values to True & convert it to 1
df_retail_txn_table
# We will first convert the DataFrame to have Boolean (True/False) instead of 1/0
df_retail_final=(df_retail_txn_table > 0)
df_retail_final
# Import necessary libraries
import pandas as pd
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules
import seaborn as sns
# Apply the Apriori algorithm to find frequent itemsets
min_support = 0.003  # Minimum support threshold - change this number and run again
frequent_itemsets = apriori(df_retail_final, min_support=min_support, use_colnames=True)
frequent_itemsets
# Discover association rules
min_confidence = 0.01  # Minimum confidence threshold
rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=min_confidence)
rules
rules.head(50)
# Let us compile a final list with some filter, calculation & sorting
final_rules=rules[(rules['lift']>1)&(rules['support']>=0.001)&(rules['confidence']<=0.09)]
# Determine number of items in X => predicting number of items in Y
final_rules=final_rules.copy() # this step creates an independent list instead of a view on rules from above
# We capture the number of items in the list of each antecedent/consequent set using 'len' function for each row
final_rules['antecedent_count']=final_rules['antecedents'].apply(len)
final_rules['consequent_count']=final_rules['consequents'].apply(len) #X

# Apply some rounding and sorting on Lift
final_rules=round(final_rules,2).sort_values(by=['lift', 'confidence'], ascending=[False, False])

# Preserve relevant columns only
final_rules=final_rules[['antecedents', 'antecedent_count', 'consequents', 'consequent_count', 'support', 'confidence', 'lift']]
final_rules
final_rules.to_csv('school_holidays.csv')
